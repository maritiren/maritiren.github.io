[{"content":"","date":null,"permalink":"/tags/appsec/","section":"Tags","summary":"","title":"AppSec"},{"content":" Heyyo This section contains whatever I think is useful when I do computer stuff.\n","date":null,"permalink":"/posts/","section":"Blog posts","summary":"","title":"Blog posts"},{"content":"","date":null,"permalink":"/tags/rapid-risk-assessment-rra/","section":"Tags","summary":"","title":"Rapid Risk Assessment (RRA)"},{"content":"","date":null,"permalink":"/tags/stride/","section":"Tags","summary":"","title":"STRIDE"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/threat-modeling/","section":"Tags","summary":"","title":"Threat Modeling"},{"content":"Note! This is still a work in progress^_^ ðŸ‘·\nOrganizations undertake various activities to enhance application security, with threat modeling being one of the most impactful activities. Others are CI pipelines with security checks, or a vulnerability management system, creating standards or maybe development guides. There are so many activities!\nThreat modeling might be one of the most important activities in application security. Still, a lot of organizations struggle to implement it and make it valuable enough for development teams. I have done a lot of thinking and some research to figure out why this is, and I have my hypothesis\u0026rsquo; on this. However, I won\u0026rsquo;t dive into that. Anyways, make sure to do your homework and ensure that you deliver something valuable in the first version. It is paramount to ask developers for their needs, their pain points, how it can be valuable for them to make this activity stick.\nThese are my own thoughts and experiences on setting up a threat modeling activity in an organization. Maybe it will come in handy for others :)\nAre you an AppSec personnel, or a security architect or anyone that has been appointed to implement activities to dev teams? Open this drop-down and read this ^_^ I want to emphasize one important thing, a misinterpretation I\u0026rsquo;ve seen and that I think is dangerous for the important work of making AppSec a more common practice among development. As someone tasked with enhancing application security, it\u0026rsquo;s easy to perceive that development teams may not be delivering sufficiently secure services. No matter how true that is, do not say that when approaching them!\nHave struggled to get the dev team\u0026rsquo;s trust? Maybe it is time to change your approach? Instead, acknowledge their skills and express your enthusiasm for collaboration. This fosters a positive environment and encourages open communication. Let the teams know that you will need their help to create something that works for them. Then ask them what they need help with or how they think things could be done differently to improve the process of delivering services with security in mind. Recognizing the pressures developers face can foster a collaborative spirit.\nHonestly, just think about it. Your expertise is security. The development landscape is shifting all the time and new development process methods, technology and expectations from developers changes all the time. Developers expected expertise is most likely so much wider than yours, and here you come adding to their workload. Not only are they expected to know their programming language, they have to get domain knowledge, know pipelines, ways of rolling out their services, cloud hosting, several kinds of testing, architecture, all should be good quality, they are already behind on technical debt due to prioritization, and now they must at a deeper level consider\u0026hellip; security!!\nWhile security is essential, the immediate value often comes from the development team\u0026rsquo;s efforts. They are the ones delivering products that drive the business forward. You are there to assist them, not to save the world from their mistakes. You are definitely not there to tell devs that they suck in security by saying that they aren\u0026rsquo;t good enough at the moment and make it seem like you are the hero assigned by the company to save the world and fix their mistakes. I am 100% sure that the company did not hire you to do that. (Yes, I\u0026rsquo;ve experienced this and after joining tons of security meetups it seems like this attitude is a little more common than it should be.)\nIt is OK to think you are a hero. I think you are! Doing this amazing work to protect users and organizations. Just keep it to yourself and make the developers feel like heroes. And yes, imo they are too!\nUltimately, your role is to equip development teams with the right tools to ensure they can deliver secure and reliable services efficiently. Tooling that should already have been there for the teams from the get-go. Tools that are necessary for dev teams to do what the job they are expected to do. Tools that are quick, easy, takes as little effort as possible. Preferably that removes workload. Find their pain points and use your expertise to figure out what tools the organization is missing that would be helpful. If that is your approach, they are more likely to welcome you. Show them that you are there for them, not for management to tick of some compliance checks.\nWhat is threat modeling? #Threat modeling is a proactive approach to identifying and mitigating potential security threats in applications.\nI often describe threat modeling as a more engaging form of risk analysis, focusing on the aspects that are more fun and valuable to developers, while streamlining the process.\nIt is also a valuable for newcomers in teams to easily understand the service they are working with, as it provides a good overview of how the service works, where the data flows, and how critical the service is.\nTo better understand threat modeling, let\u0026rsquo;s look at how Microsoft defines it:\nThreat modeling is an effective way to help secure your systems, applications, networks, and services. It\u0026rsquo;s an engineering technique that identifies potential threats and offers recommendations to help reduce risk and meet security objectives earlier in the development lifecycle.\nThreat modeling can be done both at system level and service level.\nFor those interested in a deeper dive, I encourage you to explore various threat modeling methods available online. I recommend starting with RRA and STRIDE, which are briefly mentioned in the next section.\nThreat modeling methods #No single threat modeling method suits all organizations. Tailoring the approach is key to effective threat modeling. Also, allow teams to tailor it further themselves, enabling teams to adopt methods that provide real value. It is better to use 15 minutes to discuss threats during planning than doing nothing. Although, I would advice to use a little more thorough process.\nI have previously worked with Rapid Risk Assessment (RRA) and implemented a tailored version of RRA for an organization. What I have not done is to implement the typical threat modeling method STRIDE. The way of thinking during the process is the same for both methods. The goal is the same as well. The difference is how detailed the road to get there is. Also, how much effort is needed to get started with it. Microsoft also has their own tailored version of STRIDE.\nExamples of other frameworks are Attack Trees, PASTA, DREAD and CVSS. While RRA is quick and adaptable, STRIDE and the other methods offers a more detailed framework for identifying threats.\nWhat is common for all the methods is that they do these stages:\nGather information about the service and draw a data-flow diagram Use the diagram and knowledge of the service to find threats against the service Implement the mitigations from previous step (and verify it) Rapid Risk Assessment is very well explained with examples in this video. Choosing the method for your organization #When setting up threat modeling for the first time, start by asking yourself the following questions:\nHow critical are the services? Do they need extra attention to security? How mature is my organization? Is it naturally ready to use a more detailed framework? What will the dev teams prefer? A short and concise, or a longer and detailed framework? Do we have dedicated resources to help whenever teams are conducting a threat modeling activity? If you\u0026rsquo;re dealing with military grade stuff that needs tons of security, then maybe it is better to go for a detailed framework. Then you have proper reasoning, and most likely have enough budget to make the process painless by assigning dedicated resources to both continuously improve the activity and help teams out when conducting it.\nNext step is asking the dev teams what their pain points are, and how to relieve them. What is it that makes threat modeling hard for dev teams in your organization? With this information, create a draft that avoids the pain points and includes supportive tools (see section Supportive tools to streamline the process). Then ask teams for feedback.\nWhen you have a version you believe is good enough, do several test runs with a team. Maybe for different services, or several times for the same service. Maybe guide them yourself the first time, and then make the security champion do it the next time. Tag along, be a fly on the wall. And then gather feedback and further improve the activity.\nWhen this team approves, you can now be more certain that this activity is good enough for launching.\nIn conclusion, as long as the organization doesn\u0026rsquo;t have very critical services that require extra attention to security, I recommend starting with a short and concise method like Rapid Risk Assessment and tailor it to fit your organization\u0026rsquo;s needs.\nLaunching the activity! #First off, make sure you did your homework and created a valuable activity. By launching a new activity, you are already making team\u0026rsquo;s development process more involved and stressful. Try not to lose their faith straight away.\nThen, plan a launching strategy. Maybe security champions should get training and guide their teams while you are watching and assisting? Should you do it the first time? Is it natural that the tech leads gets this training and they have backing from security champions in form of providing more security knowledge?\nAfter launching the activity, regularly check in with teams to assess improvements and gather feedback. As an AppSec admin, remember that if the process hinders more than helps, it\u0026rsquo;s time for improvement.\nI believe that most \u0026ldquo;normal\u0026rdquo; organizations aren\u0026rsquo;t mature enough for involved threat modeling processes. To begin with, the process should to be low-effort and short in order to be valuable enough for dev teams in less mature organizations. When teams are familiar to the threat modeling way of thinking, they often welcome more involved processes and some suggest doing more thorough processes themselves.\nDo dev teams have a way to provide feedback on the activities in your organization? Supportive tools to streamline the process #To streamline the process and make it low-effort, organizations should develop supportive tools for threat modeling, such as\na process description for the threat modeling process, accompanied with a guided form to perform the process a data classification table with examples, e.g. employee data is categorized as \u0026ldquo;internal\u0026rdquo; a minimum set of threat scenarios that the organization\u0026rsquo;s services should cover (this is the most valuable for orgs where teams aren\u0026rsquo;t used to threat modeling) a threat analysis, who are likely to attack? a description of the organization\u0026rsquo;s risk appetite, how much risk is the organization willing to take? a designated contact person for assistance and clarifications a person to introduce the process the first time(s) Honestly, I don\u0026rsquo;t expect all devs to read these documents. And I don\u0026rsquo;t think anyone should expect that. However, some times teams come to a point where they get uncertain. Uncertainties is a big no-no and should be eliminated! Let\u0026rsquo;s say a team is stuck discussing whether a threat scenario is in scope for the company or not. Or, whether a piece of data should be classified as internal or confidential because they don\u0026rsquo;t quite know how to classify data. Instead of having teams discussing without really knowing, which in turn can give the process negative vibes, give them the tools to look it up.\nData-flow diagrams #Data-flow diagrams are integral to most threat modeling activities. It is a helpful drawing of where the data flows in the service you are creating a threat model for. The drawing often includes trust zones and has different shapes to represent processes, data stores, external entities, data-flow and trust boundaries.\nThe Microsoft Learn path is nice to learn more about the diagrams. Microsoft outlines four depth layers for data-flow diagrams, and I concur that most data-flow diagrams should include both layers 0 and 1:\nDepth layer Title Description 0 System Starting point for any system. Data-flow diagram contains major system parts with enough context to help you understand how they work and interact with each other. 1 Process Focus on data-flow diagrams for each part of the system by using other data-flow diagrams. Use this layer for every system, especially if it handles sensitive data. The context at this layer helps you identify threats and ways to reduce or eliminate risks more efficiently. All four depth layers can be found in Learning path\u0026rsquo;s first module \u0026ldquo;Introduction to threat modeling\u0026rdquo;, at Step 1 - Design in the section \u0026ldquo;Diagram layers\u0026rdquo;.\nFocus on making it easy to understand how the data flows to see where we need to ensure protection. Keep the diagram simple; avoid overcrowding it with unnecessary details. For short and sweet threat modeling processes, the goal should be that by looking at it for a minute, you should have control of where the data flows. I\u0026rsquo;ve usually used draw.io to create these diagrams. Microsoft suggests their own tools Visio and Threat Modeling Tool.\nProcess description #In addition to a guided form that dev teams use when conducting the threat modeling activity (see section Guided form - The process itself), I believe a process description is useful.\nA good process description describes what the objective is, who should the participants be and when should the activity be performed? What is the purpose and scope? Is the activity required by compliance with regulations? What should be focused on, what teams refrain from using time on?\nOther supportive documents are also suitable here.\nGuided form - The process itself #In order to go in further details on possible ways to tailor your threat modeling process, I would like to bring up an example process and walk us through it. It is based on RRA.\nExample threat modeling form My example is heavily based of Mozilla\u0026rsquo;s RRA, but modified to how I like it. You can use it as inspiration tailoring the process to your org.\nThreat modeling for \u0026ldquo;Super cool API\u0026rdquo; # Service owner(s) Team Awesome Risk owner Mr. Risk Dude Service data classification Confidential Service description A super cool API for sending automatic emails to employees and customers Data-flow diagram (2 min) #A simple illustration, e.g. a data-flow diagram to easily see how the data flows. Add this another time, not during the process.\nData dictionary (5-10 min) # Instructions List the data types that are processed and/or stored. Make the list short and by grouping data. Classify the data according to the organization\u0026rsquo;s data classification policy (or take a look at RRA\u0026rsquo;s data classification). Set the Service Data Classification value in the table at the top of this document to the highest classification in this data dictionary. Any changes? Remember to update the list!\nData type Classification Comment Employee data Internal User data Confidential Includes dietary restrictions User data id Internal Threat scenarios (10-30 min) # Instructions Find appropriate scenarios affecting confidentiality, integrity and availability. Make sure to keep it realistic, an EMP bomb might not be in scope. Look at the example scenarios for inspiration.\n(10-25 min) Start by filling the Scenario, Driver and Assessment columns When you are done, leave the Preventive measures column for now and head to the \u0026ldquo;Concluding\u0026rdquo; section Scenario Driver Assessment Preventive measures An advice is to think \u0026ldquo;I am worried that\u0026hellip;\u0026rdquo; What\u0026rsquo;s the worst that may happen if controls aren\u0026rsquo;t implemented? Create an issue for each task, do not find or discuss solutions here (no time!) An attacker sends phishing emails I am worried that since the authorization middleware isn\u0026rsquo;t set by default on every endpoint, we have forgotten to do it on some endpoints The company looses tons of money because the CFO was phished during through our service Jira issue #1337 An attacker leaks the database, and we aren\u0026rsquo;t be able to find out how We are using the same database password for all database users. Also, I do not think we log sufficiently to say for sure where the attack originated - Our customer data leaks and we end up in the news\n- We aren\u0026rsquo;t able to find an insider who did it due to lack of logging - Jira issue #4141 (create several DB users)\n- Jira issue #4242 (add accountability logs) Concluding (3 min) #Appoint people to:\nSchedule the next meeting. Was there too little time to discuss? Don\u0026rsquo;t wait to long until next time. Otherwise, it can be a good idea to schedule it in approximately 3-6 months Create issues for scenarios in your issue management system and add their link in the Preventive measures column Prioritize the issues (or send them to whomever prioritizes) What I like to do is to create a guided document that teams fill during the process. At the top of this document, I put a table containing high level information about the service, such as the service owner(s), risk owner, service data classification, as well as a short description of the service. In some cases, I like to add the business criticality of the service, if the organization has some business critical services that are not processing critical data. These pieces of information already says a lot about the service, both for the team and for others that might need information about the service.\nThen comes the data-flow diagram, used to illustrate how the data flows through the service. The data is what we want to protect, so this diagram shows what points of the service might need more protections. These are spots that you can focus more on when utilizing the diagram later.\nNext, I like to add a table of data types that the service is process or store, and their data classification. To make it short and sweet, I prefer grouping data. For instance, instead of adding every data point for a user, write \u0026ldquo;user data\u0026rdquo; and the highest classification for the user data.\nWhen describing how to fill in the data types table, it is appropriate to add a link to the data classification document Then comes the threat modeling part of this activity, finding potential security issues in the service. In the Microsoft Learn path, two different ways of thinking are brought up, focusing on protecting the service or understanding the attacker. I like this approach, however I experience that most dev teams like to focus on protecting the system. Microsoft also states that \u0026ldquo;Microsoft product engineers mostly focus on protecting the system. Penetration testing teams focus on both.\u0026rdquo; (Wonder if an offensive security person has been working on this process at Microsoft:) ).\nWhen coming up with scenarios, I like add a scenario description, the driver for the scenario (whats the reason behind bringing it up) and thinking about how bad it might get. The latter to get an understanding of how important it is to fix the issue. For the scenarios, thinking \u0026ldquo;I am worried that\u0026hellip;\u0026rdquo; works like a charm for many people.\nThis part of the threat modeling process is probably the place you want to sit down and think very hard on what method is best for your teams. Do you want to go with STRIDE? Do you want to go with the RRA way, or the Marit (me) way? Maybe a mix between all of them? In my opinion it is all about finding the easiest approach for the teams is that still provides value. Just remember that to begin with, the teams needs to learn the way of thinking. Get a little warmed up. Then they are more likely to want to move on to a more thorough method.\nI recommend providing a list of examples scenarios or your organization\u0026rsquo;s base scenarios. This way, it is easier to get the threat modeling way of thinking started. When you have a list of scenarios, the process is almost done. Then wrapping up the process can consist of activities such as:\nSetting up a new meeting (it is easy to forget) Create issues for all scenarios to start doing research on finding security controls and fix the threats Inform the risk owner about the scenarios, it might be useful for risk analysis Inform the project manager (or whomever prioritizes issues) about the new issues Trainings for threat modeling #There is a Microsoft Learn path for threat modeling. The training explains what threat modeling is and how to draw data-flow diagrams. They have separated the workflow in four stages, design, break, fix and verify, with respective questions about the service that is great to get\nSecure Code Warrior, a training platform for secure coding has a threat modeling course. I am about to test it soon.\nSome colleagues of mine participated in a training at the Global AppSec conference.\nExample scenarios #[coming soon ^_^]\nResources # rra.rocks Microsoft Learn path Mozilla\u0026rsquo;s Rapid Risk Assessment (RRA) ","date":null,"permalink":"/posts/threat-modeling/","section":"Blog posts","summary":"","title":"Threat Modeling - a low-effort and concise approach"},{"content":" Welcome to my notebook! I realized I need to put my final notes somewhere. When I go back to something I've taken notes about, I don't remember how finished they really are. If I publish them publicly, they have to be somewhat done! ^_^ ","date":null,"permalink":"/","section":"Welcome to my notes! ðŸŽ‰","summary":"","title":"Welcome to my notes! ðŸŽ‰"},{"content":"","date":null,"permalink":"/tags/defectdojo/","section":"Tags","summary":"","title":"Defectdojo"},{"content":"DefectDojo is an OWASP (Open Worldwide Application Security Project) project for vulnerability management.\nIt is free, but also has paid SaaS solutions.\nFirst, the goal is to set up DefectDojo and receive data from Trivy reports in my Kubernetes cluster. I also want to configure DefectDojo to fit a certain organization structure and see how SBOM fits in the platform. Then, I want to add scans from pipelines.\nPrerequisites to follow these steps # A local cluster (I use Kind) The Trivy Operator running in the cluster To understand my Kubernetes setup, checkout my posts about running local Kind cluster with local registry and Flux. Installation of DefectDojo #Docker Compose #It is very easy to run DefectDojo using Docker Compose. I just cloned the repo, ran it and it was all good. Here are the instructions.\nHowever, due to local networking, it is easier to setup with Kubernetes. Therefore, Kubernetes installation and configuring is the focus here.\nKubernetes # Here\u0026rsquo;s the Kubernetes installation documentation. Note that the Kubernetes documentation is old and not maintained. It wasn\u0026rsquo;t straight forward to setup either. A downside of the Kubernetes setup is that we cannot simply add the Helm chart using Kubernets manifest files. This is what I did in the end:\nAdd cluster config to Kind and run cluster. Note! This is already done if you setup your cluster like step 1 in this post. Setup NGINX Ingress controller and apply it. I chose to just copy the files to create the Ingress controller with Flux in my local cluster. Create and apply namespace defectdojo. I chose to do so as a manifest file. Write and run bash script to setup DefectDojo with Helm locally: #!/bin/sh # 0. Clone repo if not exists echo \u0026#34;[0] Cloning repo if not exists\u0026#34; if test -d /home/maritiren/git/testing/django-defectdojo; then echo \u0026#34;django-defectdojo exists, not cloning repo\u0026#34; else echo \u0026#34;Repo doesn\u0026#39;t exist, cloning...\u0026#34; git clone https://github.com/DefectDojo/django-DefectDojo ~/git/testing/django-defectdojo fi ## 1. Fetch Helm repos echo \u0026#34;[1] Adding Defect Dojo Helm repo\u0026#34; helm repo add helm-charts \u0026#39;https://raw.githubusercontent.com/DefectDojo/django-DefectDojo/helm-charts\u0026#39; helm repo update echo \u0026#34;[2] Adding Bitnami Helm repo\u0026#34; helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update # 2. Update helm dependencies echo \u0026#34;[3] Updating Helm dependencies\u0026#34; cd /home/maritiren/git/testing/django-defectdojo helm dependency update ./helm/defectdojo # 3. Install helm chart echo \u0026#34;[4] Installing Defect Dojo with Helm\u0026#34; cd /home/maritiren/git/testing/django-defectdojo helm install \\ defectdojo \\ ./helm/defectdojo \\ -n defectdojo \\ --set django.ingress.enabled=true \\ --set django.ingress.activateTLS=false \\ --set createSecret=true \\ --set createRabbitMqSecret=true \\ --set createPostgresqlSecret=true \\ --set createMysqlSecret=false \\ --set createRedisSecret=false \\ --set \u0026#34;alternativeHosts={defectdojo-django.defectdojo.svc.cluster.local}\u0026#34; #\u0026gt; helm-install.out # setting alternative host for Trivy to send data # 4. set host value for defectdojo.default.minikube.local echo \u0026#34;[5] Adding host to /etc/hosts\u0026#34; if grep -Fxq \u0026#34;127.0.0.1 defectdojo.default.minikube.local\u0026#34; /etc/hosts then echo \u0026#34;Already added to /etc/hosts. Skipping.\u0026#34; else echo \u0026#34;Adding host to /etc/hosts to resolve host value...\u0026#34; echo \u0026#34;# Defect Dojo\u0026#34; | sudo tee -a /etc/hosts echo \u0026#34;::1 defectdojo.default.minikube.local\u0026#34; | sudo tee -a /etc/hosts echo \u0026#34;127.0.0.1 defectdojo.default.minikube.local\u0026#34; | sudo tee -a /etc/hosts fi # 5. print DefectDojo password echo \u0026#34;[6] DefectDojo admin password: $(kubectl \\ get secret defectdojo \\ --namespace=defectdojo \\ --output jsonpath=\u0026#39;{.data.DD_ADMIN_PASSWORD}\u0026#39; \\ | base64 --decode)\u0026#34; echo \u0026#34;You should now (or as soon as the Kubernetes resources are finished setting up) be able to open http://defectdojo.default.minikube.local:8080.\u0026#34; Send Trivy report data to DefectDojo #This involves setting up trivy-dojo-report-operator to automatically send report data created by the Trivy Operator to DefectDojo when they are created.\nDifferent ways of installing is described here. We are deploying with Helm, so none of the options in the docs correspond to these steps. See step 3 in this note for Helm setup. Clone repo, git clone git@github.com:telekom-mms/trivy-dojo-report-operator.git\nFetch the API key. Go to DefectDojo and click the person-icon, and then press \u0026ldquo;API v2 Key\u0026rdquo;. Then \u0026ldquo;Generate New Key\u0026rdquo; and you got it!\nAdd the API key and the DefectDojo URL to the setup of your choice configuration and apply the changes. I chose to apply the Helm chart as manifests: Note that the URL is the internal cluster URL # trivy-dojo-report-operator.yaml --- apiVersion: v1 kind: Namespace metadata: name: defectdojo-report --- apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: trivy-dojo-report-operator namespace: flux-system spec: interval: 60m url: https://telekom-mms.github.io/trivy-dojo-report-operator --- apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: trivy-dojo-report-operator namespace: defectdojo-report spec: chart: spec: chart: trivy-dojo-report-operator version: \u0026#39;0.6.2\u0026#39; sourceRef: kind: HelmRepository name: trivy-dojo-report-operator namespace: flux-system interval: 60m values: defectDojoApiCredentials: apiKey: \u0026#34;fa9b2d02...5a9c739b\u0026#34; url: \u0026#34;http://defectdojo-django.defectdojo.svc.cluster.local\u0026#34; # For internal K8s routing operator.trivyDojoReportOperator.env.defectDojoEvalEngagementName: \u0026#34;true\u0026#34; operator.trivyDojoReportOperator.env.defectDojoEvalProductName: \u0026#34;true\u0026#34; operator.trivyDojoReportOperator.env.defectDojoEvalProductTypeName: \u0026#34;true\u0026#34; operator.trivyDojoReportOperator.env.defectDojoEngagementName: \u0026#39;body[\u0026#34;report\u0026#34;][\u0026#34;artifact\u0026#34;][\u0026#34;tag\u0026#34;]\u0026#39; operator.trivyDojoReportOperator.env.defectDojoProductName: \u0026#39;body[name]\u0026#39; operator.trivyDojoReportOperator.env.defectDojoProductTypeName: \u0026#39;body[\u0026#34;namespace\u0026#34;\u0026#39; install: crds: CreateReplace createNamespace: true Something isn\u0026rsquo;t working as supposed to, with the following error message. It is a normal error message (as far as I know) when Python requests do not work correctly.\n[2024-08-02 07:00:39,022] kopf.objects [ERROR ] [flux-system/replicaset-image-reflector-controller-565565d549-manager] Handler \u0026#39;send_to_dojo\u0026#39; failed temporarily: Other error occurred: HTTPConnectionPool(host=\u0026#39;defectdojo.default.minikube.local\u0026#39;, port=80): Max retries exceeded with url: /api/v2/reimport-scan/ (Caused by NewConnectionError(\u0026#39;\u0026lt;urllib3.connection.HTTPConnection object at 0x7bb95574d430\u0026gt;: Failed to establish a new connection: [Errno 111] Connection refused\u0026#39;)). Retrying in 60 seconds Debugging #Handler 'send_to_dojo' failed temporarily # I used the incorrect DefectDojo API URL in the config. The correct one was http://defectdojo-django.defectdojo.svc.cluster.local. It was previously http://defectdojo.default.minikube.local which works from outside the cluster. However, Kubernetes didn\u0026rsquo;t route the request out of the cluster, only within. Something isn\u0026rsquo;t working as supposed to, with the following error message. It is a normal error message (as far as I know) when Python requests do not work correctly.\n[2024-08-02 07:00:39,022] kopf.objects [ERROR ] [flux-system/replicaset-image-reflector-controller-565565d549-manager] Handler \u0026#39;send_to_dojo\u0026#39; failed temporarily: Other error occurred: HTTPConnectionPool(host=\u0026#39;defectdojo.default.minikube.local\u0026#39;, port=80): Max retries exceeded with url: /api/v2/reimport-scan/ (Caused by NewConnectionError(\u0026#39;\u0026lt;urllib3.connection.HTTPConnection object at 0x7bb95574d430\u0026gt;: Failed to establish a new connection: [Errno 111] Connection refused\u0026#39;)). Retrying in 60 seconds To debug this, I created a script to verify that the DefectDojo API could receive requests:\nimport requests token = \u0026#39;your_token\u0026#39; url = \u0026#39;http://defectdojo.default.minikube.local\u0026#39; endpoint = \u0026#39;api/v2/users\u0026#39; headers = {\u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: f\u0026#34;Token {token}\u0026#34;} r = requests.get(f\u0026#34;{url}/{endpoint}\u0026#34;, headers=headers, verify=True) # set verify to False if ssl cert is self-signed for key, value in r.__dict__.items(): print(f\u0026#34;\u0026#39;{key}\u0026#39;: \u0026#39;{value}\u0026#39;\u0026#34;) print(\u0026#39;------------------\u0026#39; This simple endpoint worked just fine. Now I want to test the endpoint that the report operator is using, /api/v2/reimport-scan. To do this, we need to attach a report file, and so we need to figure out the expected format. Therefore, I looked more into the report operator code.\nThe report operator uses kopf, a Python framework for creating operators. The framework relies on creating handlers, and the report operator has a handler that fires when a new Trivy scan pod, named for instance vulnerabilityreport.aquasecurity.github.io is created:\n# handlers.py @REQUEST_TIME.time() @kopf.on.create(report.lower() + \u0026#34;.aquasecurity.github.io\u0026#34;, labels=labels) def send_to_dojo(body, meta, logger, **_): \u0026#34;\u0026#34;\u0026#34; The main function that creates a report-file from the trivy-operator vulnerabilityreport and sends it to the defectdojo instance. \u0026#34;\u0026#34;\u0026#34; ... The handler is called per report type. What\u0026rsquo;s interesting here is to see what is sent as a file to DefectDojo.\nTo do this, I had to turn on DEBUG, but there was no config option. After struggling with the updated chart not working, I tried adding the LOG_LEVEL env var to the manifest and running manifests instead. Nothing really worked\u0026hellip; Then I realized that either something magical is happening to the LOG_LEVEL (maybe through kopf is reading the environment variable and setting the level), or it is never set. So I tried to set it manually with logger.setLevel(\u0026quot;DEBUG\u0026quot;). That didn\u0026rsquo;t work either. I just need to see the object, so I ended up changing the log statement to log.info instead of log.debug. (Although I really wanted to do it the proper way, I guess I must realize when it is time to throw in the towel). The log now contains all the request data that is sent with the request.\nIn addition, I stored the report.json to the filesystem in the pod, so that I could copy it to the host with k cp -n defectdojo-report trivy-dojo-report-operator-operator-769978f8d5-rcklv:/tmp/report.json ./report.json.\nNow that I have everything I need to make the request from the host machine, I tried and it worked perfectly. Therefore, my suspicions (and fear) that this would be network related has been confirmed.\nFigure out network issue #To send requests from the Trivy Dojo Report Operator, we need to install cURL. However, we don\u0026rsquo;t have sudo on the pod. Therefore, we did the following:\npgrep kopf. All processes in Kubernetes is visible to the host machine, so we can fetch the pid of the process. In our case, the kopf process. sudo nsenter -a -t $(pgrep kopf). We use the ID to enter the Docker container namespace with nsenter. apt update \u0026amp;\u0026amp; apt install -y curl OK, now we\u0026rsquo;ve got cURL on the pod. Let\u0026rsquo;s go!\nSolution #The problem was that Kubernetes didn\u0026rsquo;t route the request out of the cluster, it deals with the routing internally. That means, the Trivy Dojo Report Operator couldn\u0026rsquo;t find the FQDN of DefectDojo, as it is running in another namespace and didn\u0026rsquo;t use the internal routing FQDN. It should be defectdojo-django.defectdojo.svc.cluster.local.\nDefectDojo doesn\u0026rsquo;t allow this sort of host by default, so we must add it to an allow list upon start of DefectDojo.\nhelm install \\ defectdojo \\ ./helm/defectdojo \\ -n defectdojo \\ --set django.ingress.enabled=true \\ --set django.ingress.activateTLS=false \\ --set createSecret=true \\ --set createRabbitMqSecret=true \\ --set createPostgresqlSecret=true \\ --set createMysqlSecret=false \\ --set createRedisSecret=false \\ --set \u0026#34;alternativeHosts={defectdojo-django.defectdojo.svc.cluster.local}\u0026#34; # \u0026lt;---- Woho! It worked! Trivy Dojo Report Operator is now sending all reports!\nResources # https://www.defectdojo.org/ https://github.com/telekom-mms/trivy-dojo-report-operator ","date":null,"permalink":"/posts/defectdojo/","section":"Blog posts","summary":"A step-by-step guide to set up DefectDojo in a local Kind cluster and receive data from the Trivy Operator reports, using the trivy-dojo-report-operator.","title":"DefectDojo with Trivy cluster reports"},{"content":"","date":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"K8s"},{"content":"","date":null,"permalink":"/tags/kind/","section":"Tags","summary":"","title":"Kind"},{"content":"","date":null,"permalink":"/tags/owasp/","section":"Tags","summary":"","title":"Owasp"},{"content":"","date":null,"permalink":"/tags/trivy/","section":"Tags","summary":"","title":"Trivy"},{"content":"","date":null,"permalink":"/tags/trivy-dojo-report-operator/","section":"Tags","summary":"","title":"Trivy-Dojo-Report-Operator"},{"content":"","date":null,"permalink":"/tags/ci/","section":"Tags","summary":"","title":"CI"},{"content":"","date":null,"permalink":"/tags/iac-scan/","section":"Tags","summary":"","title":"IaC Scan"},{"content":"","date":null,"permalink":"/tags/pipeline/","section":"Tags","summary":"","title":"Pipeline"},{"content":"I am currently working with automated security testing to get control of the known vulnerabilities in our applications. As part of this, I am scanning a Kubernetes cluster and it\u0026rsquo;s images, as well as application code. We want to cover the whole width, not only application code. Now, we look at a Infrastructure as Code (IaC) scanning tool, Trivy.\nRead more about Trivy in my other post.\nCreating a Trivy CI pipeline #GitLab CI #This pipeline is for GitLab CI when not using GitLab Ultimate. Ultimate has Trivy included in their scans. You can read more about it in the Trivy Docs.\nThe link above contains example pipelines. Here is another example, using the Trivy container only to do filesystem scan\nIf you want to follow best practice and do both image and filesystem-scan, you can use this example from the Trivy docs. stages: - Test Trivy IaC scan: stage: Test allow_failure: true # show warning, but allow merge for high severity findings image: name: docker.io/aquasec/trivy:0.56.2@sha256:26245f364b6f5d223003dc344ec1eb5eb8439052bfecb31d79aeba0c74344b3a entrypoint: [\u0026#34;\u0026#34;] variables: TRIVY_NO_PROGRESS: \u0026#34;true\u0026#34; TRIVY_CACHE_DIR: \u0026#34;.trivycache/\u0026#34; TRIVY_IGNOREFILE: \u0026#34;.trivyignore.yaml\u0026#34; cache: when: \u0026#39;always\u0026#39; # save cache even when job fails key: trivy-cache paths: - .trivycache script: - trivy --version # Clear the scan cache (keep the vuln DB) - trivy clean --scan-cache # Build the report - \u0026gt; trivy fs . --exit-code 0 --scanners=misconfig,vuln,secret --format template --template \u0026#34;@/contrib/gitlab-codequality.tpl\u0026#34; --output trivy_fs_${CI_COMMIT_SHORT_SHA}.test.json # Fail on HIGH and CRITICAL vulnerabilities - \u0026gt; trivy fs . --exit-code 1 --severity CRITICAL,HIGH artifacts: name: \u0026#34;trivy_fs_${CI_COMMIT_SHORT_SHA}.test.json\u0026#34; reports: codequality: trivy_fs_${CI_COMMIT_SHORT_SHA}.test.json expire_in: \u0026#34;30 days\u0026#34; Debugging #go not scanned properly #Some pipelines got 44 findings, while other\u0026rsquo;s had none. By looking at the pipeline logs, we figured that it wasn\u0026rsquo;t detected properly that this was a Go repo.\nRunning the commands locally also yields no findings (except one that is ignored with .trivyignore). With debug on, I found that Trivy is Skipping vulnerability scan as no version is detected for the package name=\u0026quot;my.project.com/main/module\u0026quot;.\n2024-10-03T11:37:58+02:00\tINFO\t[gomod] Detecting vulnerabilities... 2024-10-03T11:37:58+02:00\tDEBUG\t[gomod] Scanning packages for vulnerabilities\tfile_path=\u0026#34;go.mod\u0026#34; 2024-10-03T11:37:58+02:00\tDEBUG\t[gomod] Skipping vulnerability scan as no version is detected for the package name=\u0026#34;my.project.com/main/module\u0026#34; Possibilitites:\nSet an older version? An idea from this issue stating it stopped working after version 0.45.1 Maybe the cache isn\u0026rsquo;t working properly Trying an older version:\ndocker run -v $PWD:/src --workdir /src aquasec/trivy:0.45.0 -d fs . --scanners config,vuln,secret --no-progress ... 2024-10-06T15:12:43.971Z\tDEBUG\tGOPATH (/root/go/pkg/mod) not found. Need \u0026#39;go mod download\u0026#39; to fill licenses and dependency relationships 2024-10-06T15:12:43.979Z\tDEBUG\tOS is not detected. 2024-10-06T15:12:43.979Z\tDEBUG\tDetected OS: unknown 2024-10-06T15:12:43.979Z\tINFO\tNumber of language-specific files: 1 2024-10-06T15:12:43.979Z\tINFO\tDetecting gomod vulnerabilities... 2024-10-06T15:12:43.979Z\tDEBUG\tDetecting library vulnerabilities, type: gomod, path: go.mod ... Dockerfile (dockerfile) ======================= Tests: 26 (SUCCESSES: 25, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (UNKNOWN: 0, LOW: 1, MEDIUM: 0, HIGH: 0, CRITICAL: 0) ... .trivycache/policy/content/commands/kubernetes/containerNetworkInterfaceFilePermissions.yaml (secrets) ====================================================================================================== Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1 Trying the newest version at this time, 0.56.0:\ndocker run -v $PWD:/src --workdir /src aquasec/trivy:0.56.0 -d fs . --scanners misconfig,vuln,secret --no-progress We cloned the Trivy repo and logged to check whether the packages has been scanned. On version 0.56.0, they are scanned even though the main module gets the message Skipping vulnerability scan as no version is detected for the package name=\u0026quot;my.project.com/main/module\u0026quot;. However, when the scan finds all modules, it says Number of language-specific files: N, where N is more than just one.. This needs more investigation.\nFound here in the Trivy code.\nI think this might be a GitLab Runner issue. Perhaps the GitLab Runner cache. All local tests with and without Trivy cache is working as supposed to. Perhaps not, though. Still looking into it.\nThe results were inconsistent in GitLab. After clearing the pipeline cache, the pipeline findings are more consistent. However, they are not the same locally and in pipeline. Seems like due to recursive dependencies are added in the pipeline and not locally. Locally, I don\u0026rsquo;t have the The \u0026ldquo;very\u0026rdquo; old results that had bout 35-44 findings, I think is because of GitLab Runner cache of the Go path.\nRegarding the recursive dependencies, I see that the scan fetches packages from $HOME/go/pkg/mod. After running go mod download, I have other dependencies locally than what is scanned in the pipeline. That might cause different results.\nI decided to try with GitHub Actions to see if it acts weird there as well.\nThe Trivy GH Action uses Trivy version 0.53.0 By default, it doesn\u0026rsquo;t log the output from the Trivy run Husk:\nSjekk om det er Go cache Resources # https://www.aquasec.com/blog/devsecops-with-trivy-github-actions/ https://github.com/aquasecurity/trivy/discussions/5744 Trivy GitHub discussion: Intermittent missing vulnerabilities from Trivy 0.55.0 #7585 ","date":null,"permalink":"/posts/trivy-pipeline/","section":"Blog posts","summary":"","title":"Trivy in CI Pipeline"},{"content":"I am currently working with automated security testing to get control of the known vulnerabilities in our applications. As part of this, I am scanning a Kubernetes cluster and it\u0026rsquo;s images, as well as application code. We want to cover the whole width, not only application code. Now, we look at a Infrastructure as Code (IaC) scanning tool, Trivy.\nRead more about Trivy in my other post.\nTrivy has a Kubernetes operator called Trivy Operator. Advantages with using the Trivy Operator are (source) :\nTrivy Operator does background scans continuously in the cluster Trivy CLI cannot detect changes of any resources running inside the cluster Trivy Operator allows integrating with tools that can consume Kubernetes manifests as it produces reports that are CRDs Kubernetes best practice is to push information from within the cluster to tools outside rather than letting the tools pull data from the outside One downside which is not in our scope, but is worth mentioning, is that the Trivy Operator does not scan prior to deployment. Hence, it can not be used as a quality control or quality gate. Therefore, you should still run Trivy CLI in your pipelines. Prereqs to follow this guide # Kind Kubectl kubecm, strictly not necessary, but I like this tool! Helm Trivy An image that can be deployed to the cluster Info #The Trivy Operator continuously scans the Kubernetes cluster. From docs:\nThe Operator does this by watching Kubernetes for state changes and automatically triggering security scans in response. For example, a vulnerability scan is initiated when a new Pod is created. This way, users can find and view the risks that relate to different resources in a Kubernetes-native way.\nStep-by-step #Setup local cluster # Setup Flux with local registry OR setup a simple cluster\nkind create cluster Select local cluster (it is called kind-kind by default) with kubecm\nkubecm s kind-kind For a more realistic environment, run a deployment\nk apply -f ~/git/testing/flux-image-updates/clusters/my-cluster/podinfo/podinfo-deployment.yaml watch kubectl get pods (optional) push image to the local registry and create deployment for it\nTrivy needs images to scan, but there are probably already other images in your cluster. E.g. from Flux or others.\ndocker tag 0ac97f5bbbb5 localhost:5001/my-api:1.0.1 docker push localhost:5001/my-api:1.0.1 (optional) Check contents of registry\nFlux should automatically deploy new images when they get a new tag (according to the tag policy). To verify what\u0026rsquo;s in the registry, you can curl it.\nâœ— curl -X GET http://localhost:5001/v2/_catalog {\u0026#34;repositories\u0026#34;:[\u0026#34;my-api\u0026#34;,\u0026#34;hello-app\u0026#34;,\u0026#34;podinfo\u0026#34;]} **strong text** âœ— curl -X GET http://localhost:5001/v2/podinfo/tags/list {\u0026#34;name\u0026#34;:\u0026#34;podinfo\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;5.0.7\u0026#34;,\u0026#34;5.0.3\u0026#34;,\u0026#34;5.0.5\u0026#34;,\u0026#34;5.0.0\u0026#34;,\u0026#34;5.0.4\u0026#34;,\u0026#34;5.0.6\u0026#34;]} (optional) Test Trivy Operator locally #https://aquasecurity.github.io/trivy-operator/latest/\nI do this to get a better feeling of how Trivy works and how it should look in the cluster. You can install the Trivy Operator using a YAML manifest file, or as a Helm Chart. We will do the latter. Steps from the docs:\nOption 1: Install from traditional Helm Chart repository\nAdd the Aqua chart repository: helm repo add aqua https://aquasecurity.github.io/helm-charts/ helm repo update Install the Helm Chart: helm install trivy-operator aqua/trivy-operator \\ --namespace trivy-system \\ --create-namespace \\ --version 0.21.0 Installing the operator yields the following output:\nâœ— helm install trivy-operator aqua/trivy-operator \\ --namespace trivy-system \\ --create-namespace \\ --version 0.21.0 NAME: trivy-operator LAST DEPLOYED: Wed Mar 27 09:14:18 2024 NAMESPACE: trivy-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You have installed Trivy Operator in the trivy-system namespace. It is configured to discover Kubernetes workloads and resources in all namespace(s). Inspect created VulnerabilityReports by: kubectl get vulnerabilityreports --all-namespaces -o wide Inspect created ConfigAuditReports by: kubectl get configauditreports --all-namespaces -o wide Inspect the work log of trivy-operator by: kubectl logs -n trivy-system deployment/trivy-operator Running these commands, we see that the operator is starting making reports. I am interested to see if the podinfo deployment has any reports.\nThe instructions above are from the \u0026ldquo;Home\u0026rdquo; page of the docs, while there are also more options in the Helm installation page.\nPlay with the in-cluster API #\u0026hellip; to get an overview of have the tool works. Is it even worth installing in the cluster?\nTrivy creates several types of reports:\nVulnerabilityReport ConfigAuditReport ExposedSecretReport RbacAssessmentReport InfraAssessmentReport ClusterComplianceReport ClusterVulnerabilityReport SbomReport Get an overview #To get an overview of all findings, we can use the reports as shown in the Helm Install output above:\nâœ— kubectl get vulnerabilityreports --all-namespaces -o wide NAMESPACE NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN flux-system replicaset-helm-controller-58d5cc6f5b-manager fluxcd/helm-controller v0.37.2 Trivy 92m 0 1 11 0 0 flux-system replicaset-image-automation-controller-654dc4897-manager fluxcd/image-automation-controller v0.37.0 Trivy 93m 0 1 8 0 0 flux-system replicaset-image-reflector-controller-8498c88d9-manager fluxcd/image-reflector-controller v0.31.1 Trivy 92m 0 0 9 0 0 ... âœ— kubectl get configauditreports --all-namespaces -o wide NAMESPACE NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW default replicaset-podinfo-5d869859bd Trivy 94m 0 2 3 9 default service-kubernetes Trivy 94m 0 0 0 0 my-api replicaset-my-api-7cc565547 Trivy 94m 0 1 2 9 flux-system networkpolicy-allow-egress Trivy 94m 0 0 0 0 ... Dive deeper into findings #To checkout the findings, run the following commands\nâœ— kubectl describe vulnerabilityreport my-vulnerability-report -n default âœ— kubectl describe configauditreport my-configaudit-report -n default My pod had the following HIGH finding:\nCategory: Kubernetes Security Check Check ID: KSV118 Description: Security context controls the allocation of security parameters for the pod/container/volume, ensuring the appropriate level of protection. Relying on default security context may expose vulnerabilities to potential attacks that rely on privileged access. Messages: replicaset my-api-7cc565547 in my-api namespace is using the default security context, which allows root privileges Remediation: To enhance security, it is strongly recommended not to rely on the default security context. Instead, it is advisable to exp licitly define the required security parameters (such as runAsNonRoot, capabilities, readOnlyRootFilesystem, etc.) within the security context . Severity: HIGH Success: false Title: Default security context configured Setup Trivy Operator in production #Trivy Operator manifest files #https://aquasecurity.github.io/trivy/v0.50/tutorials/kubernetes/gitops/\nOkay, so I think this will give value. Especially in a scenario where Kubernetes is used for standard applications in an organisation. Then we have centralized image scanning and can report known vulnerabilities without the teams having to set up anything themselves.\nSo let\u0026rsquo;s look into how to add the Trivy Operator to a cluster in production.\n--- apiVersion: v1 kind: Namespace metadata: name: trivy-system --- apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository metadata: name: trivy-operator namespace: flux-system spec: interval: 60m type: oci url: oci://ghcr.io/aquasecurity/helm-charts --- apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease metadata: name: trivy-operator namespace: trivy-system spec: chart: spec: chart: trivy-operator version: 0.21.0 sourceRef: kind: HelmRepository name: trivy-operator namespace: flux-system interval: 60m values: trivy: resources: limits: memory: 1500M # Default of ?? wasn\u0026#39;t enough, causing OOMKilled workload containers ignoreUnfixed: true operator: scanJobsConcurrentLimit: 2 # Default of 10 used too much RAM at once for Nodes, causing OOMkilled workload containers install: crds: CreateReplace createNamespace: false Configure Calico network policy #If you are using Calico or other network management tools and run the manifests above, you will most likely get the following error or something similar: unable to run trivy operator: failed getting configmap: trivy-operator: Get \u0026quot;https://10.0.0.1:443/api/v1/namespaces/trivy-system/configmaps/trivy-operator\u0026quot;: dial tcp 10.0.0.1:443: i/o timeout.\nThis means that you need to add a network policy. Trivy requires two network accesses:\nAccess to the K8s API Access to the vulnerability database Adhering to the principle of least privilege is quite hard here. In the network policy, only IP ranges can be set. Using Azure, one can set more tailored rules in front. However, those rules apply to the whole cluster, not one namespace or Kubernetes resource.\nHere is an example of a network policy for Calico to allow Trivy access to the K8s API:\n--- apiVersion: projectcalico.org/v3 kind: NetworkPolicy metadata: name: allow-trivy-operator-egress namespace: trivy-system spec: types: - Egress egress: - action: Allow # allow k8s API calls destination: services: name: kubernetes namespace: default - action: Allow # allow vulnerability database fetch destination: ports: - \u0026#34;443\u0026#34; nets: - 0.0.0.0/0 protocol: TCP To check the calico network policy when it has been applied:\ncalicoctl get networkpolicy allow-trivy-operator-egress -o yaml -n trivy-system --allow-version-mismatch Tolerate node taints #https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\nIf your cluster has taints on nodes, you will see that the Trivy node collector isn\u0026rsquo;t running correctly. You can check by first finding the node collector name (list all resources in the namespace and you will have it), and then run kubectl describe on it. The events will show what is wrong, e.g. FailedScheduling with the message 0/7 nodes are available: 1 node(s) had untolerated taint .....\nk describe pod/node-collector-677f9fb5b8-jc6tw -n trivy-system Find the taints on the nodes in your cluster:\nâžœ ~ kubectl get nodes -o json | jq \u0026#39;.items[] | {name: .metadata.name, taints: .spec.taints}\u0026#39; Then you can add it to the HelmRelease as values. Note that it is not common pod tolerations you must configure, but tell the Trivy Operator through values.triyvOperator.scanJobTolerations which tolerations the node-collector pod should have.\nWhat happens when setting common tolerations Updating the tolerations, the node-collector pod didn\u0026rsquo;t get them applied, only the operator pod (checking with k describe pod/node-collector-some-id and same for pod/trivy-operator). I found an issue and deleted all files to reset, but it didn\u0026rsquo;t work. With some more research, I found that I must set the tolerations in the Helm values\n... values: trivy: ignoreUnfixed: true trivyOperator: scanJobTolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; In version 0.21.1, it was supported to add tolerations to NodeCollector. At that point, we started getting the same toleration error messages and the NodeCollector timed out. Looking at the HelmChart Artifact Hub, we fixed by adding tolerations to the node collector as well:\n... values: trivy: ignoreUnfixed: true trivyOperator: scanJobTolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeCollector: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; Done!\nAdd image scanning #Trivy scans images by default. If it is not working, make sure you allow network to fetch the vulnerability database, as well as allowing network to fetch images from your repository. Maybe this will help:\nhttps://aquasecurity.github.io/trivy-operator/latest/docs/vulnerability-scanning/private-registries/ https://aquasecurity.github.io/trivy-operator/v0.19.0/tutorials/private-registries/ Create reports #Coming soon maybe.\nGrafana dashboard #Documentation at https://aquasecurity.github.io/trivy-operator/v0.22.0/tutorials/grafana-dashboard/#using-the-grafana-helm-chart.\nThis was an easy fix, however it took a little time to figure out how to use the gnetId to create image through Terraform. I made a PR to the Trivy docs, so it should be documented now.\nUseful commands #Trivy Operator logs #From K8s:\nk logs -n trivy-system deployment/trivy-operator See all trivy-system resource (except from network policy):\nk get all -n trivy-system Flux reconcile logs #flux logs --namespace flux-system --since=1h -f flux logs --namespace flux-system --since=1h -f --kind=kustomization flux logs --namespace flux-system --since=1h -f --kind=kustomization --name=trivy-prereqs See new commits as they are detected:\nflux logs --namespace flux-system --since=1h -f --kind=gitrepository Delete/restart the operator #After doing lots of testing, you might want to delete the operator and install it again to see that a clean install works. You can do like this:\nkubectl delete all --all -n trivy-system Flux might automatically reinstall. If not, you can run\nflux reconcile kustomization flux-system --with-source -n flux-system or maybe\nflux reconcile helmrelease trivy-operator -n trivy-system Or just delete the files and see that the resources are gone, and then put them back.\nDelete all reports #kubectl delete exposedsecretreport --all --all-namespaces And then same for other reports, such as vulnerabilityreport.\nImprovements and further thoughts #There are several improvements to be done here. Here are some of my thoughts:\nCreate reports, e.g. monthly reports or immediate alerts for critical findings Scan regularly Use hash of versions and do not scan that very image again every time it appears in the cluster. E.g. job images. Debugging #SBOM decode error: failed to decode: multiple OS components are not supported #{ \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2024-04-08T10:04:45Z\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;reconciler.scan job\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Scan job container\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;trivy-system/scan-vulnerabilityreport-6cccfb67dd\u0026#34;, \u0026#34;container\u0026#34;: \u0026#34;k8s-cluster\u0026#34;, \u0026#34;status.reason\u0026#34;: \u0026#34;Error\u0026#34;, \u0026#34;status.message\u0026#34;: \u0026#34;2024-04-08T10:04:37.564Z\\t\\u001b[31mFATAL\\u001b[0m\\tsbom scan error: scan error: scan failed: failed analysis: SBOM decode error: failed to decode: failed to decode components: multiple OS components are not supported\\n\u0026#34;, \u0026#34;stacktrace\u0026#34;: \u0026#34;github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).completedContainers\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:353\\ngithub.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).SetupWithManager.(*ScanJobController).reconcileJobs.func1\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:80\\nsigs.k8s.io/controller-runtime/pkg/reconcile.Func.Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/reconcile/reconcile.go:113\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:227\u0026#34; } Haven\u0026rsquo;t looked into this yet.\nscan error: unable to initialize an image scanner: remote error (image fetch) #This problem is because GETing the URL provides a bad response.\n{ \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2024-04-23T03:07:55Z\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;reconciler.scan job\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Scan job container\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;trivy-system/scan-vulnerabilityreport-7f665c795b\u0026#34;, \u0026#34;container\u0026#34;: \u0026#34;calico-windows-upgrade\u0026#34;, \u0026#34;status.reason\u0026#34;: \u0026#34;Error\u0026#34;, \u0026#34;status.message\u0026#34;: \u0026#34;2024-04-23T03:07:53.141Z\\t\\u001b[31mFATAL\\u001b[0m\\timage scan error: scan error: unable to initialize a scanner: unable to initialize an image scanner: 4 errors occurred:\\n\\t* docker error: unable to inspect the image (mcr.microsoft.com/oss/calico/windows-upgrade:v3.26.3): Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\\n\\t* containerd error: containerd socket not found: /run/containerd/containerd.sock\\n\\t* podman error: unable to initialize Podman client: no podman socket found: stat podman/podman.sock: no such file or directory\\n\\t* remote error: GET https://mcr.microsoft.com/v2/oss/calico/windows-upgrade/manifests/v3.26.3: MANIFEST_UNKNOWN: manifest tagged by \\\u0026#34;v3.26.3\\\u0026#34; is not found; map[Tag:v3.26.3]\\n\\n\\n\u0026#34;, \u0026#34;stacktrace\u0026#34;: \u0026#34;github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).completedContainers\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:353\\ngithub.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).SetupWithManager.(*ScanJobController).reconcileJobs.func1\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:80\\nsigs.k8s.io/controller-runtime/pkg/reconcile.Func.Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/reconcile/reconcile.go:113\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.3/pkg/internal/controller/controller.go:227\u0026#34; } I don\u0026rsquo;t need Windows upgrades at all, so I am thinking of options to handle this:\nDisable scanning with Trivy with label target workload (turn off DaemonSet-reports) namespace (turn off reports for all calico-system resources) Disable calico-windows-upgrade DaemonSet Don\u0026rsquo;t have a solution just yet. Didn\u0026rsquo;t prioritise this because triggers this error is a deprecated feature that will be removed in the future.\nScanning Windows images is not supported #Pretty self-explanatory.\n{ \u0026#34;level\u0026#34;: \u0026#34;info\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2024-04-08T10:07:35Z\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;reconciler.scan job\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Scan job container\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;trivy-system/scan-vulnerabilityreport-7f4d674d74\u0026#34;, \u0026#34;container\u0026#34;: \u0026#34;init\u0026#34;, \u0026#34;status.reason\u0026#34;: \u0026#34;Error\u0026#34;, \u0026#34;status.message\u0026#34;: \u0026#34;Scanning Windows images is not supported.\u0026#34; } Scan job - OOMKilled âœ… #{ \u0026#34;level\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2024-04-08T10:08:16Z\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;reconciler.scan job\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Scan job container\u0026#34;, \u0026#34;job\u0026#34;: \u0026#34;trivy-system/scan-vulnerabilityreport-5c498d8bc6\u0026#34;, \u0026#34;container\u0026#34;: \u0026#34;prometheus\u0026#34;, \u0026#34;status.reason\u0026#34;: \u0026#34;OOMKilled\u0026#34;, \u0026#34;status.message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stacktrace\u0026#34;: \u0026#34;github.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).completedContainers\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:353\\ngithub.com/aquasecurity/trivy-operator/pkg/vulnerabilityreport/controller.(*ScanJobController).SetupWithManager.(*ScanJobController).reconcileJobs.func1\\n\\t/home/runner/work/trivy-operator/trivy-operator/pkg/vulnerabilityreport/controller/scanjob.go:80\\nsigs.k8s.io/controller-runtime/pkg/reconcile.Func.Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/reconcile/reconcile.go:113\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.17.2/pkg/internal/controller/controller.go:227\u0026#34; } The default values for RAM wasn\u0026rsquo;t enough. We gave a little more request and limit for the scanners and this solved the problem. In addition there were 10 reports generated simultanuously. We changed it to 2 to give the nodes a little room. It is shown under spec.values in the manifest file.\nTOO MANY REQUESTS âœ… # This is an ongoing issue, so Trivy might solve this without us having to take action. However, it has been going on a while now. There are two options for this issue;\nWait until the Trivy maintainers have fixed the issue Setup a mirror for the vulnerability database At some point, we suddenly got the following error:\n2024-10-03T07:31:26Z\tFATAL\tFatal error\tinit error: DB error: failed to download vulnerability DB: database download error: oci download error: failed to fetch the layer: GET https://ghcr.io/v2/aquasecurity/trivy-db/blobs/sha256:77a50f405854d311fdf062f2d7edf3c04c63e2f5d218751a29125431376757a1: TOOMANYREQUESTS: retry-after: 600.129Âµs, allowed: 44000/minute I found out why this happens from a discussions thread in the Trivy repo: \u0026ldquo;This is happening just because Trivy has too many users and reached the rate limits. \u0026ldquo;. Apparently, the GitHub container registry, ghcr.io, introduced rate limiting which ended up with this issue. Or, it is just Trivy that has gotten too many users.\nSo, in theory setting up cache for the vulnerability database should help. However, if we need to update the database often, it doesn\u0026rsquo;t really help too much. It certainly doesn\u0026rsquo;t solve the problem entirely.\nThat leaves us two other options;\nSetting up our own package registry to mirror the vulnerability database. A comment on the option above states that this adds security risk. It definitely force us to maintain another thing and we risk not having the latest discovered vulnerabilities in our mirrored version. If the database doesn\u0026rsquo;t get updated, we might just end up allowing another Log4j affect our systems. Anyways, I think this is a good option if this is still an issue next week. If you are patient and OK with rerunning your failing pipelines, then a solution could be to wait for the Trivy maintainers to fix the problem. They are looking into the issue and have already pushed small improvements very quickly to remediate. I am sure they are looking into it and trying their best to fix the issue quickly. Later, I found this announcement about the issue.\nResources # https://aquasecurity.github.io/trivy/v0.50/docs/target/kubernetes/ https://aquasecurity.github.io/trivy-operator/latest/ https://www.aquasec.com/blog/vulnerability-scanning-trivy-vs-the-trivy-operator/ https://github.com/aquasecurity/trivy/discussions/4905 https://github.com/aquasecurity/trivy/discussions/4499 https://aquasecurity.github.io/trivy/v0.17.2/private-registries/ K8s Lens: https://docs.k8slens.dev/ Lens extension: https://aquasecurity.github.io/trivy-operator/v0.10.1/tutorials/integrations/lens/ https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/ https://github.com/aquasecurity/trivy-operator/issues/1659 ","date":null,"permalink":"/posts/trivy-operator/","section":"Blog posts","summary":"A step-by-step guide to setup the Trivy Operator in Kubernetes.","title":"Trivy in Kubernetes"},{"content":"","date":null,"permalink":"/tags/quality/","section":"Tags","summary":"","title":"Quality"},{"content":"I am currently working with automated security testing to get control of the known vulnerabilities in our applications. As part of this, I am scanning a Kubernetes cluster and it\u0026rsquo;s images, as well as application code. We want to cover the whole width, not only application code. Now, we look at a Infrastructure as Code (IaC) scanning tool, Trivy.\nNote! When it says \u0026ldquo;All-in-one\u0026rdquo; and \u0026ldquo;vulnerability scanning\u0026rdquo;, it does not mean application code scanning. The focus is on IaC, so find something else in addition for your application code!_ I am very happy with Trivy scanning. It is a nice free tool that is easy to use in GitHub, GitLab, locally, and just wherever you want.\nI am just hoping that this is not another Semgrep, a nice free tool that ends up as a paid tool after it becomes popular.\nAdvantages #There are many pitfalls when writing IaC, and scanning it helps us in several ways. We don\u0026rsquo;t need to know everything, and we don\u0026rsquo;t need to pay constant attention to new vulnerabilities. We get more control of what we create, and create more predictable applications. We can be more confident on what we deliver when we run scans like this.\nWhen \u0026amp; where to scan # At your local environment, before pushing to remote (VS Code extension, pre-commit hooks, etc.) As automatic PR review. Although, I only believe in using this as a quality gate as long as it is possible to buypass it. In case of critical scenarios, such as fixing down-time or a critical bug. Regularly in the production environment, e.g. every saturday evening. What scans to run #We should run different Trivy scans. Which scans to run differ from application to application. However, in general, I believe these scans should be run:\nConfiguration file scan, to scan your IaC configuration files for known vulnerabilities Secrets scan, to check if there are any secrets present in your code Dependency scan, to scan your dependency versions for known vulnerabilities Image scan, to scan images. In my opinion, this should be done both on PR review and regularly in prod. K8s cluster scan, if you have a K8s cluster, this is a super way to scan images in the Kubernets cluster when they appear. The three first scans can be done in Trivy filesystem scan, configuring it with `\u0026ndash;scanners=misconfig,secret,vuln. I would do both the filesystem scan and image scan as CI checks in a PR review.\nDockerfile scan vs image scan # Do we really need both? Yes! I asked my skilled colleagues this very questions, and can say that without doubt, you should do both!\nThis is section based on my colleague\u0026rsquo;s explanation.\nDockerfile scan checks the instructions used to build an image, such as the base image, package installations, config files and scripts. Also, whether you use best practices for versions, users with too high privileges or exposure of sensitive data, e.g. insecure mounting of fileshare.\nImage scanning finds known vulnerabilities in all installed packages, including the base image and dependencies that aren\u0026rsquo;t directly visible in the Dockerfiel. Image scanning gives more of a depth of all the packages and third-party dependencies. E.g. using an image that is running an old version of Python with known vulnerabilities won\u0026rsquo;t be found by the Dockerfile scan, but it will be discovered by the image scan. Only if the usage is explicitly written in the Dockerfile.\nCaching #Trivy fetches new vulns on an interval, every 12th hour.\nStep-by-step guides for different scans #I am going to write different guides on applying Trivy in projects. Here are the ones I have so far:\nTrivy in Kubernetes Trivy in CI pipelines (coming soon) ","date":null,"permalink":"/posts/trivy/","section":"Blog posts","summary":"Contains several guides for applying Trivy to your project","title":"Trivy Infrastructure as Code (IaC) Scanning"},{"content":"","date":null,"permalink":"/tags/flux/","section":"Tags","summary":"","title":"Flux"},{"content":"This post show every step of setting up a Kind cluster with Flux for automatic image updates from a local registry.\nPrerequisites # Kind Kubectl GitHub account 1. Setup Kind cluster with local registry # The script below also includes Ingress config for hosting DefectDojo in your cluster. If you don\u0026rsquo;t want to setup DefectDojo later, remove the whole nodes: section. Find the script in the post Kind Cluster with Local Registry. You may run this script even if you already have running local registry. It didn\u0026rsquo;t do anything to my images at least.\ncd ~/git/flux-image-updates/clusters ./create-kind-cluster-with-registry.sh 2. Run Flux #Following the Flux guide \u0026ldquo;Automate image updates to Git\u0026rdquo;, I setup everything as follows:\nStart by adding your GitHub credentials as environment variables. PAT token can be found at Settings\u0026gt; Developer Settings\u0026gt; Tokens (classic). It requires the repo scope.\nexport GITHUB_TOKEN=ghp_gCVsYEC... export GITHUB_USER=maritiren You can run this although you already have an existing GitHub repo for Flux (called flux-image-updates). This does not overwrite manifest files.\nflux bootstrap github \\ --components-extra=image-reflector-controller,image-automation-controller \\ --owner=$GITHUB_USER \\ --repository=flux-image-updates \\ --branch=main \\ --path=clusters/my-cluster \\ --read-write-key \\ --personal 3. Setup secrets for namespaces to fetch images #Add secret for the namespaces you use to fetch an image:\nk create secret docker-registry regcred --docker-server=\u0026#34;kind-registry:5000\u0026#34; --docker-username=myuser --docker-password=myuser -n flux-system k create secret docker-registry regcred2 --docker-server=\u0026#34;kind-registry:5000\u0026#34; --docker-username=myuser --docker-password=myuser -n my-api Watch the image repositories to see status:\nwatch flux get image repository --all-namespaces And then reconcile the image repositories:\nflux reconcile image repository my-api -n my-api flux reconcile image repository podinfo -n flux-system 4. Push image to local registry #docker tag 0ac97f5bbbb5 localhost:5001/my-api:1.0.0 docker push localhost:5001/my-api:1.0.0 podinfo: https://hub.docker.com/r/stefanprodan/podinfo\ndocker pull stefanprodan/podinfo 5. (optional) Check contents of registry #Flux should automatically deploy new images when they get a new tag (according to the tag policy). To verify what\u0026rsquo;s in the registry, you can curl it.\nâœ— curl -X GET http://localhost:5001/v2/_catalog {\u0026#34;repositories\u0026#34;:[\u0026#34;my-api\u0026#34;,\u0026#34;hello-app\u0026#34;,\u0026#34;podinfo\u0026#34;]} **strong text** âœ— curl -X GET http://localhost:5001/v2/podinfo/tags/list {\u0026#34;name\u0026#34;:\u0026#34;podinfo\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;5.0.7\u0026#34;,\u0026#34;5.0.3\u0026#34;,\u0026#34;5.0.5\u0026#34;,\u0026#34;5.0.0\u0026#34;,\u0026#34;5.0.4\u0026#34;,\u0026#34;5.0.6\u0026#34;]} Cleanup #Only delete the cluster:\nkind cluster delete --name \u0026#34;kind-kind\u0026#34; Deleting both the cluster and registry. (From https://github.com/piyushjajoo/kind-with-local-registry-and-ingress/blob/master/destroy.sh):\n#!/bin/sh set -o errexit # delete kind cluster echo \u0026#34;deleting kind cluster\u0026#34; kind delete cluster --name \u0026#34;kind-kind\u0026#34; # delete registry echo \u0026#34;deleting registry\u0026#34; docker rm -f $(docker ps -a | grep registry | awk -F \u0026#39; \u0026#39; \u0026#39;{print $1}\u0026#39;) Debugging # K8s can connect to local registry, but Flux can\u0026rsquo;t K8s can\u0026rsquo;t connect to local registry, but Flux can Completely recreate deployment K8s can connect to local registry, but Flux can\u0026rsquo;t # Solution: Use port 5000 in the Deployment manifest files and other places in K8s. Even thouhg the port of the registry is 5001. This was one of the first huge issues I got while setting this up. I used so many hours debugging this. I cannot provide a complete overview of the debugging as I didn\u0026rsquo;t take notes at that time. However, I remember what fixed access for Flux in the end.\nAt first, the problem was that it didn\u0026rsquo;t really connect to the local registry at all. At that point the error message was something like \u0026ldquo;Connection refused\u0026rdquo;, so I thought I was going towards the local registry but had some authentication problem. Turned out I wasn\u0026rsquo;t even sending requests to the registry. Sorry, I don\u0026rsquo;t have the answer to this case here, probably changed some ports or DNS name of some sorts.\nHaving the local registry receiving my requests, I started getting the error http: server gave HTTP response to HTTPS client.\nI tried a couple of things, on of them was to add credentials to the docker config file and restart dockerd. A simple mistake in the config was made, which in turn made dockerd go bananas. It restarted all the time, wouldn\u0026rsquo;t allow us to reset and we ended up allowing more restarts in order to roll back the changes. This didn\u0026rsquo;t solve anything either.\nThen in the end, it turned out the error was that we used port 5001, while Kubernetes was connecting to the registry using port 5000. In other words, the solution was to use port 5000 in the Deployment manifest files and otherwise in Kubernetes. We still aren\u0026rsquo;t quite sure why, but assume this is due to how we connected the registry container and the cluster using docker network connect. Otherwise, from the outside of the cluster, we reach the registry using localhost:5001. PRETTY DARN CONFUSING! Localhost here and localhost there, K8s DNS name here. Good luck understanding this!\nK8s can\u0026rsquo;t connect to local registry, but Flux can #There are many (or few - depending on how you see things) options on what could be wrong. Here are a couple of different things that might help with this issue:\nSolution: What worked for me was to change the containerd configs in the cluster startup script. Changing containerd configs # https://github.com/kubernetes-sigs/kind/issues/2604#issuecomment-1041314277 Remove the existing config line and add this instead, in the cluster startup script. This makes some parts of the Kind script unuseful (the part of setting containerd settings in each node). You might change this either in cluster-config.yaml or in the Kind startup script.\ncontainerdConfigPatches: - |- [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;kind-registry:5000\u0026#34;] endpoint = [\u0026#34;http://kind-registry:5000\u0026#34;] Enable this in the Docker deamon (probably requires restart of deamon) #This didn\u0026rsquo;t work really, but maybe? Maybe I didn\u0026rsquo;t do it properly. Didn\u0026rsquo;t work for me, though.\nâœ— sudo cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;localhost:5001\u0026#34;] } Set network connection with Docker #This should have been done in the script from Kind, but it was suggested as a solution.\ndocker network connect kind kind-registry Completely recreate a deployment #âœ— k delete deployment my-api -n my-api âœ— flux reconcile kustomization flux-system --with-source Resources # https://fluxcd.io/flux/guides/image-update https://kind.sigs.k8s.io/docs/user/local-registry/ https://hackernoon.com/kubernetes-cluster-setup-with-a-local-registry-and-ingress-in-docker-using-kind https://stackoverflow.com/a/3175054 ","date":null,"permalink":"/posts/flux-kind-localregistry/","section":"Blog posts","summary":"A step-by-step guide for setting up Flux in a Kind cluster with a local registry for automatic updates of Docker images.","title":"Flux with Local Registry"},{"content":"","date":null,"permalink":"/tags/local-registry/","section":"Tags","summary":"","title":"Local Registry"},{"content":"Setting up a local registry for Kind clusters requires settings to be applied when creating the cluster. It should work to just run the script provided in the Kind docs.\nHowever, I also need it to work with Flux which did not work out of the box. Therefore, I\u0026rsquo;ve modified the script from the Kind docs to work with Flux. Honestly, I don\u0026rsquo;t remember exactly why, but it is in step 2 and it is related to the FQDN used by the cluster to access the registry.\nHere is the script:\n#!/bin/sh # from https://kind.sigs.k8s.io/docs/user/local-registry/ set -o errexit # 1. Create registry container unless it already exists reg_name=\u0026#39;kind-registry\u0026#39; reg_port=\u0026#39;5001\u0026#39; if [ \u0026#34;$(docker inspect -f \u0026#39;{{.State.Running}}\u0026#39; \u0026#34;${reg_name}\u0026#34; 2\u0026gt;/dev/null || true)\u0026#34; != \u0026#39;true\u0026#39; ]; then docker run \\ -d --restart=always -p \u0026#34;127.0.0.1:${reg_port}:5000\u0026#34; --network bridge --name \u0026#34;${reg_name}\u0026#34; \\ registry:2 fi # 2. Create kind cluster with containerd registry config dir enabled # TODO: kind will eventually enable this by default and this patch will # be unnecessary. # # See: # https://github.com/kubernetes-sigs/kind/issues/2875 # https://github.com/containerd/containerd/blob/main/docs/cri/config.md#registry-configuration # See: https://github.com/containerd/containerd/blob/main/docs/hosts.md # # Original: # [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] # config_path = \u0026#34;/etc/containerd/certs.d\u0026#34; # # Ingress config (for DefectDojo app): # https://kind.sigs.k8s.io/docs/user/ingress/#create-cluster cat \u0026lt;\u0026lt;EOF | kind create cluster --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 containerdConfigPatches: - |- [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;kind-registry:5000\u0026#34;] endpoint = [\u0026#34;http://kind-registry:5000\u0026#34;] nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \u0026#34;ingress-ready=true\u0026#34; extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF # 3. Add the registry config to the nodes # # This is necessary because localhost resolves to loopback addresses that are # network-namespace local. # In other words: localhost in the container is not localhost on the host. # # We want a consistent name that works from both ends, so we tell containerd to # alias localhost:${reg_port} to the registry container when pulling images REGISTRY_DIR=\u0026#34;/etc/containerd/certs.d/localhost:${reg_port}\u0026#34; for node in $(kind get nodes); do docker exec \u0026#34;${node}\u0026#34; mkdir -p \u0026#34;${REGISTRY_DIR}\u0026#34; cat \u0026lt;\u0026lt;EOF | docker exec -i \u0026#34;${node}\u0026#34; cp /dev/stdin \u0026#34;${REGISTRY_DIR}/hosts.toml\u0026#34; [host.\u0026#34;http://${reg_name}:5000\u0026#34;] EOF done # 4. Connect the registry to the cluster network if not already connected # This allows kind to bootstrap the network but ensures they\u0026#39;re on the same network if [ \u0026#34;$(docker inspect -f=\u0026#39;{{json .NetworkSettings.Networks.kind}}\u0026#39; \u0026#34;${reg_name}\u0026#34;)\u0026#34; = \u0026#39;null\u0026#39; ]; then docker network connect \u0026#34;kind\u0026#34; \u0026#34;${reg_name}\u0026#34; fi # 5. Document the local registry # https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/generic/1755-communicating-a-local-registry cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: local-registry-hosting namespace: kube-public data: localRegistryHosting.v1: | host: \u0026#34;localhost:${reg_port}\u0026#34; help: \u0026#34;https://kind.sigs.k8s.io/docs/user/local-registry/\u0026#34; EOF ","date":null,"permalink":"/posts/kind-localregistry/","section":"Blog posts","summary":"A script to set up Kind and a local registry, with containerd patches to make it work for Flux","title":"Kind with Local Registry for Flux"},{"content":" Blip blop ","date":"1 January 0001","permalink":"/about/","section":"Welcome to my notes! ðŸŽ‰","summary":"","title":"About the author"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]